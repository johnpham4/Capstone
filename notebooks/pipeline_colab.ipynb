{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWdb9Dcl-x8v",
        "outputId": "605f4dc4-fc33-4d3f-e8fe-723f72b0364e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Capstone'...\n",
            "remote: Enumerating objects: 6907, done.\u001b[K\n",
            "remote: Counting objects: 100% (407/407), done.\u001b[K\n",
            "remote: Compressing objects: 100% (332/332), done.\u001b[K\n",
            "remote: Total 6907 (delta 103), reused 360 (delta 70), pack-reused 6500 (from 3)\u001b[K\n",
            "Receiving objects: 100% (6907/6907), 506.98 MiB | 39.07 MiB/s, done.\n",
            "Resolving deltas: 100% (164/164), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/johnpham4/Capstone.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zDUa5cr_SAQ",
        "outputId": "ffa5b1b8-5fb1-4395-f22f-3d75d74a9e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Capstone\n"
          ]
        }
      ],
      "source": [
        "%cd Capstone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQyJsOYr_bTx",
        "outputId": "fbeb0166-e74d-4352-d61f-3a9a4ff8aa1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: python-doctr 1.0.0 does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m844.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m853.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.9/444.9 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.2/978.2 kB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.6/525.6 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for geouni (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-adk 1.21.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.45.3 which is incompatible.\n",
            "rasterio 1.4.4 requires click!=8.2.*,>=4.0, but you have click 8.2.1 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "mcp 1.24.0 requires pyjwt[crypto]>=2.10.1, but you have pyjwt 2.7.0 which is incompatible.\n",
            "sse-starlette 3.0.4 requires starlette>=0.49.1, but you have starlette 0.45.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cmRS3x-Evs-",
        "outputId": "592b48eb-7885-467d-889d-5169ca2f4944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/803.3 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.3/803.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/241.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "zenml 0.91.2 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
            "google-adk 1.21.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.45.3 which is incompatible.\n",
            "google-adk 1.21.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.\n",
            "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.1.23 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "langgraph-checkpoint 3.0.1 requires langchain-core>=0.2.38, but you have langchain-core 0.1.23 which is incompatible.\n",
            "db-dtypes 1.4.4 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "xarray 2025.12.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain==0.0.354 langchain-community==0.0.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-UJCFnL_fAM",
        "outputId": "0e229bf1-c34d-4173-83fd-8f2c9469b651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mThe `zenml connect` command is deprecated and will be removed in a future \u001b[0m\n",
            "\u001b[33mrelease. Please use the `zenml login` command instead. \u001b[0m\n",
            "\u001b[2;37mCalling `zenml login`\u001b[0m\u001b[2;33m...\u001b[0m\n",
            "Please enter the API key for the ZenML server: \n",
            "\u001b[37mInitializing the ZenML global configuration version to 0.91.2\u001b[0m\n",
            "\u001b[2;37mAuthenticating to ZenML server \u001b[0m\n",
            "\u001b[37m'https://victoria-communicable-sometimes.ngrok-free.dev'\u001b[0m\u001b[2;37m using an API key\u001b[0m\u001b[2;33m...\u001b[0m\n",
            "\u001b[37mSetting the global active project to 'default'.\u001b[0m\n",
            "\u001b[33mSetting the global active stack to default.\u001b[0m\n",
            "\u001b[37mUpdated the global store configuration.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!zenml connect \\\n",
        "  --url https://victoria-communicable-sometimes.ngrok-free.dev \\\n",
        "  --api-key ZENKEY_eyJpZCI6ImUxOGYwNjQ1LWRjZmEtNGQwNS04NWNkLTVkZGNjYzlmZDg5NSIsImtleSI6IjQ3YmU1MGQ5NWNiOWU4NmY2YTI5NWYxNGY5ZjM4NmE3M2IzYWIwZTZhYTk4NDA2M2E5MzI3OWUwYTU5YWI4ZTgifQ=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQmjok_HCI0r",
        "outputId": "4f16ae37-19a2-4432-820d-ac5dac8b2509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2;37m-----ZenML Client Status-----\u001b[0m\n",
            "\u001b[2;37mConnected to a remote ZenML server: \u001b[0m\n",
            "\u001b[2;37m`\u001b[0m\u001b]8;id=517150;https://victoria-communicable-sometimes.ngrok-free.dev\u001b\\\u001b[2;4;94mhttps://victoria-communicable-sometimes.ngrok-free.dev\u001b[0m\u001b]8;;\u001b\\\u001b[2;4;94m`\u001b[0m\n",
            "\u001b[2;37m  Dashboard: \u001b[0m\u001b]8;id=240877;https://victoria-communicable-sometimes.ngrok-free.dev\u001b\\\u001b[2;4;94mhttps://victoria-communicable-sometimes.ngrok-free.dev\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;37m  API: \u001b[0m\u001b]8;id=505564;https://victoria-communicable-sometimes.ngrok-free.dev/docs\u001b\\\u001b[2;4;94mhttps://victoria-communicable-sometimes.ngrok-free.dev\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;37m  Server status: \u001b[0m\u001b[37m'available'\u001b[0m\n",
            "\u001b[2;37m  Server authentication: API key\u001b[0m\n",
            "\u001b[2;37m  The active user is: \u001b[0m\u001b[37m'finetune'\u001b[0m\n",
            "\u001b[2;37m  The active project is: \u001b[0m\u001b[37m'default'\u001b[0m\u001b[2;37m \u001b[0m\u001b[1;2;37m(\u001b[0m\u001b[2;37mglobal\u001b[0m\u001b[1;2;37m)\u001b[0m\n",
            "\u001b[2;37m  The active stack is: \u001b[0m\u001b[37m'default'\u001b[0m\u001b[2;37m \u001b[0m\u001b[1;2;37m(\u001b[0m\u001b[2;37mglobal\u001b[0m\u001b[1;2;37m)\u001b[0m\n",
            "\u001b[2;37mUsing configuration from: \u001b[0m\u001b[37m'/root/.config/zenml'\u001b[0m\n",
            "\u001b[2;37mLocal store files are located at: \u001b[0m\u001b[37m'/root/.config/zenml/local_stores'\u001b[0m\n",
            "\n",
            "\u001b[2;37m-----Local ZenML Server Status-----\u001b[0m\n",
            "\u001b[2;37mThe local server has not been started.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!zenml status"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m tools.run --run-inference --no-cache"
      ],
      "metadata": {
        "id": "XVAeyOe7s0vV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m tools.run --run-train --no-cache"
      ],
      "metadata": {
        "id": "bijwxRH6gH9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f54e54-5293-4a6d-d163-022257dc0de4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[37mNumExpr defaulting to 2 threads.\u001b[0m\n",
            "\u001b[37mTensorFlow version 2.19.0 available.\u001b[0m\n",
            "\u001b[37mJAX version 0.7.2 available.\u001b[0m\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766483634.440681    4321 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766483634.500881    4321 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766483634.946067    4321 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766483634.946115    4321 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766483634.946121    4321 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766483634.946125    4321 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32m2025-12-23 09:54:03.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipelines.training\u001b[0m:\u001b[36mtraining_pipeline\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mStarting training pipeline\u001b[0m\n",
            "\u001b[32m2025-12-23 09:54:03.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipelines.training\u001b[0m:\u001b[36mtraining_pipeline\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mMerging LoRA adapter\u001b[0m\n",
            "\u001b[32m2025-12-23 09:54:03.040\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mpipelines.training\u001b[0m:\u001b[36mtraining_pipeline\u001b[0m:\u001b[36m56\u001b[0m - \u001b[32m\u001b[1mPipeline completed\u001b[0m\n",
            "\u001b[37mInitiating a new run for the pipeline: \u001b[0m\u001b[38;5;105mtraining_pipeline\u001b[37m.\u001b[0m\n",
            "\u001b[37mCaching is disabled by default for \u001b[0m\u001b[38;5;105mtraining_pipeline\u001b[37m.\u001b[0m\n",
            "\u001b[37mUsing user: \u001b[0m\u001b[38;5;105mfinetune\u001b[37m\u001b[0m\n",
            "\u001b[37mUsing stack: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
            "\u001b[37m  orchestrator: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
            "\u001b[37m  artifact_store: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
            "\u001b[37m  deployer: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
            "\u001b[37mDashboard URL for Pipeline Run: \u001b[0m\u001b[34mhttps://victoria-communicable-sometimes.ngrok-free.dev/projects/default/runs/03d023ea-e6f0-4d8e-b819-3ec9a505a7c2\u001b[37m\u001b[0m\n",
            "\u001b[37mStep \u001b[0m\u001b[38;5;105mload_model_step\u001b[37m has started.\u001b[0m\n",
            "\u001b[32m2025-12-23 09:54:26.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.train.load_model\u001b[0m:\u001b[36mload_model_step\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mLoading model\u001b[0m\n",
            "\u001b[32m2025-12-23 09:54:26.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLoading LLM: Qwen/Qwen2.5-3B-Instruct\u001b[0m\n",
            "config.json: 100% 661/661 [00:00<00:00, 4.46MB/s]\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "model.safetensors.index.json: 35.6kB [00:00, 65.6MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/3.97G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 1.75M/3.97G [00:01<49:54, 1.32MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 4.59M/3.97G [00:01<19:49, 3.33MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 33.5M/3.97G [00:01<02:05, 31.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 43.4M/3.97G [00:02<02:02, 32.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 56.8M/3.97G [00:02<01:59, 32.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 72.5M/3.97G [00:02<01:25, 45.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 84.6M/3.97G [00:02<01:36, 40.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 91.1M/3.97G [00:03<01:36, 40.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 99.4M/3.97G [00:03<01:24, 45.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 129M/3.97G [00:04<01:56, 32.9MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 140M/3.97G [00:04<02:15, 28.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 148M/3.97G [00:05<02:00, 31.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 158M/3.97G [00:05<02:13, 28.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 176M/3.97G [00:05<01:29, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 214M/3.97G [00:05<00:49, 75.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 227M/3.97G [00:06<01:35, 39.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 242M/3.97G [00:06<01:19, 46.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 251M/3.97G [00:07<01:17, 47.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 282M/3.97G [00:07<00:51, 72.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 357M/3.97G [00:07<00:35, 103MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 411M/3.97G [00:07<00:27, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 461M/3.97G [00:08<00:25, 136MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 523M/3.97G [00:08<00:24, 139MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 603M/3.97G [00:08<00:17, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 628M/3.97G [00:09<00:16, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 696M/3.97G [00:11<00:58, 56.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 725M/3.97G [00:11<00:52, 61.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 790M/3.97G [00:12<00:37, 84.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 846M/3.97G [00:12<00:28, 110MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 870M/3.97G [00:12<00:26, 118MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 903M/3.97G [00:12<00:27, 112MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 927M/3.97G [00:13<00:25, 121MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 995M/3.97G [00:13<00:20, 143MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.06G/3.97G [00:13<00:17, 164MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.12G/3.97G [00:14<00:28, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.14G/3.97G [00:14<00:26, 107MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.21G/3.97G [00:15<00:20, 137MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.27G/3.97G [00:15<00:17, 155MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.31G/3.97G [00:15<00:20, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.38G/3.97G [00:16<00:15, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.45G/3.97G [00:16<00:12, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.52G/3.97G [00:16<00:12, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.55G/3.97G [00:16<00:12, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.57G/3.97G [00:17<00:14, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 1.61G/3.97G [00:17<00:22, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 1.68G/3.97G [00:18<00:14, 154MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 1.73G/3.97G [00:18<00:15, 146MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 1.80G/3.97G [00:21<00:48, 44.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 1.90G/3.97G [00:22<00:30, 69.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 1.96G/3.97G [00:22<00:22, 88.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.02G/3.97G [00:26<00:49, 39.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.15G/3.97G [00:26<00:26, 67.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.18G/3.97G [00:26<00:23, 77.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.25G/3.97G [00:26<00:18, 91.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.33G/3.97G [00:32<00:48, 33.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 2.41G/3.97G [00:32<00:33, 46.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 2.44G/3.97G [00:32<00:28, 52.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 2.47G/3.97G [00:32<00:25, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 2.54G/3.97G [00:33<00:17, 83.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 2.60G/3.97G [00:33<00:14, 91.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 2.64G/3.97G [00:33<00:13, 102MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 2.70G/3.97G [00:35<00:22, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 2.76G/3.97G [00:36<00:16, 72.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 2.83G/3.97G [00:36<00:12, 91.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 2.90G/3.97G [00:36<00:08, 127MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 2.94G/3.97G [00:37<00:07, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.00G/3.97G [00:37<00:06, 142MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.06G/3.97G [00:37<00:04, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.13G/3.97G [00:37<00:04, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 3.20G/3.97G [00:38<00:03, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 3.26G/3.97G [00:38<00:03, 229MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 3.33G/3.97G [00:38<00:03, 163MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 3.40G/3.97G [00:42<00:11, 50.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 3.48G/3.97G [00:42<00:06, 73.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 3.53G/3.97G [00:42<00:05, 85.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 3.59G/3.97G [00:43<00:03, 107MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 3.66G/3.97G [00:43<00:02, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 3.73G/3.97G [00:44<00:02, 113MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 3.78G/3.97G [00:47<00:04, 44.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 3.83G/3.97G [00:47<00:02, 58.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 3.90G/3.97G [00:47<00:00, 75.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 3.97G/3.97G [00:48<00:00, 82.4MB/s]\n",
            "Downloading shards:  50% 1/2 [00:48<00:48, 48.41s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.20G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 9.20M/2.20G [00:00<03:44, 9.77MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 20.2M/2.20G [00:01<01:35, 22.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 30.0M/2.20G [00:01<01:12, 30.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 55.3M/2.20G [00:01<00:35, 60.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 70.1M/2.20G [00:01<00:28, 74.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 85.9M/2.20G [00:01<00:25, 82.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 121M/2.20G [00:01<00:15, 135MB/s]  \u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 145M/2.20G [00:01<00:15, 133MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 181M/2.20G [00:02<00:15, 130MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 197M/2.20G [00:02<00:16, 120MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 211M/2.20G [00:02<00:18, 111MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 228M/2.20G [00:02<00:20, 97.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 247M/2.20G [00:03<00:23, 83.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 281M/2.20G [00:03<00:18, 105MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 326M/2.20G [00:03<00:12, 150MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 351M/2.20G [00:03<00:13, 137MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 376M/2.20G [00:03<00:13, 131MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 390M/2.20G [00:04<00:17, 106MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 410M/2.20G [00:04<00:24, 73.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 475M/2.20G [00:04<00:13, 133MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 507M/2.20G [00:05<00:15, 113MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 538M/2.20G [00:05<00:19, 84.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 605M/2.20G [00:06<00:13, 119MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 640M/2.20G [00:06<00:17, 89.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 669M/2.20G [00:06<00:14, 104MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 691M/2.20G [00:07<00:14, 106MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 754M/2.20G [00:07<00:12, 119MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 821M/2.20G [00:07<00:09, 148MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 888M/2.20G [00:10<00:23, 54.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 984M/2.20G [00:10<00:14, 86.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.05G/2.20G [00:10<00:10, 111MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.12G/2.20G [00:14<00:24, 44.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.21G/2.20G [00:14<00:15, 65.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.26G/2.20G [00:15<00:12, 77.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.31G/2.20G [00:20<00:32, 27.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.37G/2.20G [00:20<00:21, 38.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.47G/2.20G [00:20<00:11, 61.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.51G/2.20G [00:21<00:09, 70.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.56G/2.20G [00:21<00:07, 82.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.63G/2.20G [00:21<00:05, 112MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 1.69G/2.20G [00:21<00:03, 141MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 1.76G/2.20G [00:21<00:02, 163MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 1.83G/2.20G [00:22<00:02, 186MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 1.89G/2.20G [00:22<00:01, 197MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 1.95G/2.20G [00:22<00:01, 174MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.01G/2.20G [00:23<00:01, 160MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.07G/2.20G [00:23<00:00, 211MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.14G/2.20G [00:23<00:00, 179MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.20G/2.20G [00:24<00:00, 91.2MB/s]\n",
            "Downloading shards: 100% 2/2 [01:12<00:00, 36.38s/it]\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "\u001b[37m[load_model_step] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set \u001b[0m\u001b[38;5;105mmax_memory\u001b[37m in to a higher value to use more memory (at your own risk).\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:25<00:00, 12.84s/it]\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 1.46MB/s]\n",
            "tokenizer_config.json: 7.30kB [00:00, 25.7MB/s]\n",
            "vocab.json: 2.78MB [00:00, 57.0MB/s]\n",
            "merges.txt: 1.67MB [00:00, 131MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 193MB/s]\n",
            "\u001b[32m2025-12-23 09:56:11.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mLoading VQ-VAE: JO-KU/Geo-MAGVIT\u001b[0m\n",
            "\u001b[32m2025-12-23 09:56:11.397\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m61\u001b[0m - \u001b[32m\u001b[1mModel loaded successfully\u001b[0m\n",
            "[load_model_step] trainable params: 3,686,400 || all params: 3,089,625,088 || trainable%: 0.1193\n",
            "\u001b[33m[load_model_step] No materializer is registered for type \u001b[0m\u001b[38;5;105m<class 'llm_engineering.applications.training.model_trainer.ModelTrainer'>\u001b[33m, so the default Pickle materializer was used. Pickle is not production ready and should only be used for prototyping as the artifacts cannot be loaded when running with a different Python version. Please consider implementing a custom materializer for type \u001b[0m\u001b[38;5;105m<class 'llm_engineering.applications.training.model_trainer.ModelTrainer'>\u001b[33m according to the instructions at \u001b[0m\u001b[34mhttps://docs.zenml.io/concepts/artifacts/materializers\u001b[33m\u001b[0m\n",
            "\u001b[37mStep \u001b[0m\u001b[38;5;105mload_model_step\u001b[37m has finished in \u001b[0m\u001b[38;5;105m3m53s\u001b[37m.\u001b[0m\n",
            "\u001b[37mStep \u001b[0m\u001b[38;5;105mtrain_step\u001b[37m has started.\u001b[0m\n",
            "\u001b[32m2025-12-23 09:58:56.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.train.train_model\u001b[0m:\u001b[36mtrain_step\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mLoading datasets\u001b[0m\n",
            "\u001b[32m2025-12-23 09:58:56.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mLoading dataset from /content/Capstone/data/line_segments/dataset_cached.json\u001b[0m\n",
            "\u001b[32m2025-12-23 09:58:56.770\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m74\u001b[0m - \u001b[32m\u001b[1mLoaded 200 training samples\u001b[0m\n",
            "\u001b[32m2025-12-23 09:58:56.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.train.train_model\u001b[0m:\u001b[36mtrain_step\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mStarting training\u001b[0m\n",
            "\u001b[32m2025-12-23 09:58:56.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mStarting training with cached tokens\u001b[0m\n",
            "\u001b[33m[train_step] /content/Capstone/llm_engineering/applications/training/model_trainer.py:110: FutureWarning: \u001b[0m\u001b[38;5;105mtokenizer\u001b[33m is deprecated and will be removed in version 5.0.0 for \u001b[0m\u001b[38;5;105mTrainer.__init__\u001b[33m. Use \u001b[0m\u001b[38;5;105mprocessing_class\u001b[33m instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[0m\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "  0% 0/90 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "\u001b[33m[train_step] /usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "\u001b[0m\n",
            "  1% 1/90 [00:09<14:39,  9.89s/it]\u001b[32m2025-12-23 09:59:07.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=1 loss=7.6458 lr=1.98e-04\u001b[0m\n",
            "{'loss': 7.6458, 'grad_norm': 22.666316986083984, 'learning_rate': 0.00019777777777777778, 'epoch': 0.16}\n",
            "  2% 2/90 [00:18<13:00,  8.87s/it]\u001b[32m2025-12-23 09:59:15.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=2 loss=6.4076 lr=1.96e-04\u001b[0m\n",
            "{'loss': 6.4076, 'grad_norm': 14.379677772521973, 'learning_rate': 0.00019555555555555556, 'epoch': 0.32}\n",
            "  3% 3/90 [00:26<12:28,  8.60s/it]\u001b[32m2025-12-23 09:59:23.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=3 loss=5.3348 lr=1.93e-04\u001b[0m\n",
            "{'loss': 5.3348, 'grad_norm': 9.106481552124023, 'learning_rate': 0.00019333333333333333, 'epoch': 0.48}\n",
            "  4% 4/90 [00:34<12:12,  8.52s/it]\u001b[32m2025-12-23 09:59:32.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=4 loss=4.6379 lr=1.91e-04\u001b[0m\n",
            "{'loss': 4.6379, 'grad_norm': 22.976953506469727, 'learning_rate': 0.00019111111111111114, 'epoch': 0.64}\n",
            "  6% 5/90 [00:43<11:58,  8.45s/it]\u001b[32m2025-12-23 09:59:40.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=5 loss=4.1933 lr=1.89e-04\u001b[0m\n",
            "{'loss': 4.1933, 'grad_norm': 7.998983860015869, 'learning_rate': 0.00018888888888888888, 'epoch': 0.8}\n",
            "  7% 6/90 [00:51<11:52,  8.49s/it]\u001b[32m2025-12-23 09:59:49.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=6 loss=3.9223 lr=1.87e-04\u001b[0m\n",
            "{'loss': 3.9223, 'grad_norm': 3.501246452331543, 'learning_rate': 0.0001866666666666667, 'epoch': 0.96}\n",
            "  8% 7/90 [00:53<08:52,  6.41s/it]\u001b[32m2025-12-23 09:59:51.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=7 loss=3.6944 lr=1.84e-04\u001b[0m\n",
            "{'loss': 3.6944, 'grad_norm': 3.58544659614563, 'learning_rate': 0.00018444444444444446, 'epoch': 1.0}\n",
            "  9% 8/90 [01:02<09:43,  7.12s/it]\u001b[32m2025-12-23 09:59:59.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=8 loss=3.4888 lr=1.82e-04\u001b[0m\n",
            "{'loss': 3.4888, 'grad_norm': 3.3858211040496826, 'learning_rate': 0.00018222222222222224, 'epoch': 1.16}\n",
            " 10% 9/90 [01:11<10:19,  7.65s/it]\u001b[32m2025-12-23 10:00:08.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=9 loss=3.2499 lr=1.80e-04\u001b[0m\n",
            "{'loss': 3.2499, 'grad_norm': 3.1547231674194336, 'learning_rate': 0.00018, 'epoch': 1.32}\n",
            " 11% 10/90 [01:19<10:37,  7.96s/it]\u001b[32m2025-12-23 10:00:17.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=10 loss=3.0108 lr=1.78e-04\u001b[0m\n",
            "{'loss': 3.0108, 'grad_norm': 3.379591941833496, 'learning_rate': 0.00017777777777777779, 'epoch': 1.48}\n",
            " 12% 11/90 [01:28<10:47,  8.20s/it]\u001b[32m2025-12-23 10:00:26.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=11 loss=2.8057 lr=1.76e-04\u001b[0m\n",
            "{'loss': 2.8057, 'grad_norm': 3.8987009525299072, 'learning_rate': 0.00017555555555555556, 'epoch': 1.64}\n",
            " 13% 12/90 [01:37<10:50,  8.34s/it]\u001b[32m2025-12-23 10:00:34.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=12 loss=2.6020 lr=1.73e-04\u001b[0m\n",
            "{'loss': 2.602, 'grad_norm': 3.7070369720458984, 'learning_rate': 0.00017333333333333334, 'epoch': 1.8}\n",
            " 14% 13/90 [01:45<10:50,  8.45s/it]\u001b[32m2025-12-23 10:00:43.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=13 loss=2.3883 lr=1.71e-04\u001b[0m\n",
            "{'loss': 2.3883, 'grad_norm': 4.542987823486328, 'learning_rate': 0.0001711111111111111, 'epoch': 1.96}\n",
            " 16% 14/90 [01:48<08:17,  6.55s/it]\u001b[32m2025-12-23 10:00:45.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=14 loss=2.2201 lr=1.69e-04\u001b[0m\n",
            "{'loss': 2.2201, 'grad_norm': 2.6978652477264404, 'learning_rate': 0.00016888888888888889, 'epoch': 2.0}\n",
            " 17% 15/90 [01:56<08:54,  7.12s/it]\u001b[32m2025-12-23 10:00:54.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=15 loss=2.0463 lr=1.67e-04\u001b[0m\n",
            "{'loss': 2.0463, 'grad_norm': 2.625208616256714, 'learning_rate': 0.0001666666666666667, 'epoch': 2.16}\n",
            " 18% 16/90 [02:04<09:16,  7.52s/it]\u001b[32m2025-12-23 10:01:02.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=16 loss=1.8978 lr=1.64e-04\u001b[0m\n",
            "{'loss': 1.8978, 'grad_norm': 3.6908133029937744, 'learning_rate': 0.00016444444444444444, 'epoch': 2.32}\n",
            " 19% 17/90 [02:13<09:33,  7.86s/it]\u001b[32m2025-12-23 10:01:11.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=17 loss=1.8060 lr=1.64e-04\u001b[0m\n",
            "{'loss': 1.806, 'grad_norm': nan, 'learning_rate': 0.00016444444444444444, 'epoch': 2.48}\n",
            " 20% 18/90 [02:22<09:42,  8.10s/it]\u001b[32m2025-12-23 10:01:19.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=18 loss=1.8188 lr=1.64e-04\u001b[0m\n",
            "{'loss': 1.8188, 'grad_norm': nan, 'learning_rate': 0.00016444444444444444, 'epoch': 2.64}\n",
            " 21% 19/90 [02:30<09:46,  8.26s/it]\u001b[32m2025-12-23 10:01:28.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=19 loss=1.8193 lr=1.64e-04\u001b[0m\n",
            "{'loss': 1.8193, 'grad_norm': nan, 'learning_rate': 0.00016444444444444444, 'epoch': 2.8}\n",
            " 22% 20/90 [02:39<09:46,  8.38s/it]\u001b[32m2025-12-23 10:01:37.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=20 loss=1.8119 lr=1.64e-04\u001b[0m\n",
            "{'loss': 1.8119, 'grad_norm': nan, 'learning_rate': 0.00016444444444444444, 'epoch': 2.96}\n",
            " 23% 21/90 [02:41<07:30,  6.53s/it]\u001b[32m2025-12-23 10:01:39.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=21 loss=1.8278 lr=1.64e-04\u001b[0m\n",
            "{'loss': 1.8278, 'grad_norm': nan, 'learning_rate': 0.00016444444444444444, 'epoch': 3.0}\n",
            " 24% 22/90 [02:50<08:09,  7.21s/it]\u001b[32m2025-12-23 10:01:48.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=22 loss=1.8246 lr=1.62e-04\u001b[0m\n",
            "{'loss': 1.8246, 'grad_norm': 16.56608009338379, 'learning_rate': 0.00016222222222222224, 'epoch': 3.16}\n",
            " 26% 23/90 [02:59<08:31,  7.64s/it]\u001b[32m2025-12-23 10:01:56.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=23 loss=1.6959 lr=1.60e-04\u001b[0m\n",
            "{'loss': 1.6959, 'grad_norm': 8.239326477050781, 'learning_rate': 0.00016, 'epoch': 3.32}\n",
            " 27% 24/90 [03:07<08:44,  7.94s/it]\u001b[32m2025-12-23 10:02:05.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=24 loss=1.5830 lr=1.58e-04\u001b[0m\n",
            "{'loss': 1.583, 'grad_norm': 2.7993967533111572, 'learning_rate': 0.0001577777777777778, 'epoch': 3.48}\n",
            " 28% 25/90 [03:16<08:48,  8.14s/it]\u001b[32m2025-12-23 10:02:13.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=25 loss=1.4668 lr=1.56e-04\u001b[0m\n",
            "{'loss': 1.4668, 'grad_norm': 2.5408782958984375, 'learning_rate': 0.00015555555555555556, 'epoch': 3.64}\n",
            " 29% 26/90 [03:25<08:50,  8.29s/it]\u001b[32m2025-12-23 10:02:22.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=26 loss=1.3430 lr=1.53e-04\u001b[0m\n",
            "{'loss': 1.343, 'grad_norm': 2.0168545246124268, 'learning_rate': 0.00015333333333333334, 'epoch': 3.8}\n",
            " 30% 27/90 [03:33<08:50,  8.41s/it]\u001b[32m2025-12-23 10:02:31.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=27 loss=1.2599 lr=1.51e-04\u001b[0m\n",
            "{'loss': 1.2599, 'grad_norm': 1.6087257862091064, 'learning_rate': 0.0001511111111111111, 'epoch': 3.96}\n",
            " 31% 28/90 [03:35<06:43,  6.50s/it]\u001b[32m2025-12-23 10:02:33.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=28 loss=1.1826 lr=1.49e-04\u001b[0m\n",
            "{'loss': 1.1826, 'grad_norm': 1.6001311540603638, 'learning_rate': 0.0001488888888888889, 'epoch': 4.0}\n",
            " 32% 29/90 [03:44<07:14,  7.12s/it]\u001b[32m2025-12-23 10:02:41.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=29 loss=1.1380 lr=1.47e-04\u001b[0m\n",
            "{'loss': 1.138, 'grad_norm': 1.1148040294647217, 'learning_rate': 0.00014666666666666666, 'epoch': 4.16}\n",
            " 33% 30/90 [03:52<07:31,  7.53s/it]\u001b[32m2025-12-23 10:02:50.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=30 loss=1.0979 lr=1.44e-04\u001b[0m\n",
            "{'loss': 1.0979, 'grad_norm': 1.2039268016815186, 'learning_rate': 0.00014444444444444444, 'epoch': 4.32}\n",
            " 34% 31/90 [04:01<07:42,  7.84s/it]\u001b[32m2025-12-23 10:02:58.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=31 loss=1.0496 lr=1.42e-04\u001b[0m\n",
            "{'loss': 1.0496, 'grad_norm': 0.8605096340179443, 'learning_rate': 0.00014222222222222224, 'epoch': 4.48}\n",
            " 36% 32/90 [04:10<07:49,  8.09s/it]\u001b[32m2025-12-23 10:03:07.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=32 loss=1.0216 lr=1.40e-04\u001b[0m\n",
            "{'loss': 1.0216, 'grad_norm': 0.7740644216537476, 'learning_rate': 0.00014, 'epoch': 4.64}\n",
            " 37% 33/90 [04:18<07:50,  8.26s/it]\u001b[32m2025-12-23 10:03:16.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=33 loss=0.9939 lr=1.38e-04\u001b[0m\n",
            "{'loss': 0.9939, 'grad_norm': 0.7400802969932556, 'learning_rate': 0.0001377777777777778, 'epoch': 4.8}\n",
            " 38% 34/90 [04:27<07:49,  8.38s/it]\u001b[32m2025-12-23 10:03:24.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=34 loss=0.9700 lr=1.36e-04\u001b[0m\n",
            "{'loss': 0.97, 'grad_norm': 0.7112725973129272, 'learning_rate': 0.00013555555555555556, 'epoch': 4.96}\n",
            " 39% 35/90 [04:29<05:58,  6.51s/it]\u001b[32m2025-12-23 10:03:27.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=35 loss=0.9516 lr=1.33e-04\u001b[0m\n",
            "{'loss': 0.9516, 'grad_norm': 0.763859212398529, 'learning_rate': 0.00013333333333333334, 'epoch': 5.0}\n",
            " 40% 36/90 [04:38<06:25,  7.14s/it]\u001b[32m2025-12-23 10:03:35.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=36 loss=0.9281 lr=1.31e-04\u001b[0m\n",
            "{'loss': 0.9281, 'grad_norm': 0.6698034405708313, 'learning_rate': 0.00013111111111111111, 'epoch': 5.16}\n",
            " 41% 37/90 [04:46<06:41,  7.58s/it]\u001b[32m2025-12-23 10:03:44.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=37 loss=0.9043 lr=1.29e-04\u001b[0m\n",
            "{'loss': 0.9043, 'grad_norm': 0.5592167377471924, 'learning_rate': 0.00012888888888888892, 'epoch': 5.32}\n",
            " 42% 38/90 [04:55<06:49,  7.87s/it]\u001b[32m2025-12-23 10:03:52.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=38 loss=0.8890 lr=1.27e-04\u001b[0m\n",
            "{'loss': 0.889, 'grad_norm': 0.515816330909729, 'learning_rate': 0.00012666666666666666, 'epoch': 5.48}\n",
            " 43% 39/90 [05:04<06:52,  8.09s/it]\u001b[32m2025-12-23 10:04:01.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=39 loss=0.8773 lr=1.24e-04\u001b[0m\n",
            "{'loss': 0.8773, 'grad_norm': 0.4403950572013855, 'learning_rate': 0.00012444444444444444, 'epoch': 5.64}\n",
            " 44% 40/90 [05:12<06:53,  8.26s/it]\u001b[32m2025-12-23 10:04:10.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=40 loss=0.8661 lr=1.22e-04\u001b[0m\n",
            "{'loss': 0.8661, 'grad_norm': 0.4513234794139862, 'learning_rate': 0.00012222222222222224, 'epoch': 5.8}\n",
            " 46% 41/90 [05:21<06:49,  8.35s/it]\u001b[32m2025-12-23 10:04:18.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=41 loss=0.8559 lr=1.20e-04\u001b[0m\n",
            "{'loss': 0.8559, 'grad_norm': 0.4223308265209198, 'learning_rate': 0.00012, 'epoch': 5.96}\n",
            " 47% 42/90 [05:23<05:11,  6.49s/it]\u001b[32m2025-12-23 10:04:20.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=42 loss=0.8512 lr=1.18e-04\u001b[0m\n",
            "{'loss': 0.8512, 'grad_norm': 0.45212504267692566, 'learning_rate': 0.00011777777777777779, 'epoch': 6.0}\n",
            " 48% 43/90 [05:31<05:33,  7.09s/it]\u001b[32m2025-12-23 10:04:29.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=43 loss=0.8407 lr=1.16e-04\u001b[0m\n",
            "{'loss': 0.8407, 'grad_norm': 0.45606890320777893, 'learning_rate': 0.00011555555555555555, 'epoch': 6.16}\n",
            " 49% 44/90 [05:40<05:47,  7.55s/it]\u001b[32m2025-12-23 10:04:37.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=44 loss=0.8322 lr=1.13e-04\u001b[0m\n",
            "{'loss': 0.8322, 'grad_norm': 0.35896697640419006, 'learning_rate': 0.00011333333333333334, 'epoch': 6.32}\n",
            " 50% 45/90 [05:49<05:54,  7.87s/it]\u001b[32m2025-12-23 10:04:46.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=45 loss=0.8247 lr=1.11e-04\u001b[0m\n",
            "{'loss': 0.8247, 'grad_norm': 0.29860323667526245, 'learning_rate': 0.00011111111111111112, 'epoch': 6.48}\n",
            " 51% 46/90 [05:57<05:56,  8.10s/it]\u001b[32m2025-12-23 10:04:55.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=46 loss=0.8236 lr=1.09e-04\u001b[0m\n",
            "{'loss': 0.8236, 'grad_norm': 0.29152289032936096, 'learning_rate': 0.00010888888888888889, 'epoch': 6.64}\n",
            " 52% 47/90 [06:06<05:55,  8.26s/it]\u001b[32m2025-12-23 10:05:03.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=47 loss=0.8148 lr=1.07e-04\u001b[0m\n",
            "{'loss': 0.8148, 'grad_norm': 0.2679860293865204, 'learning_rate': 0.00010666666666666667, 'epoch': 6.8}\n",
            " 53% 48/90 [06:15<05:51,  8.37s/it]\u001b[32m2025-12-23 10:05:12.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=48 loss=0.8139 lr=1.04e-04\u001b[0m\n",
            "{'loss': 0.8139, 'grad_norm': 0.4052569568157196, 'learning_rate': 0.00010444444444444445, 'epoch': 6.96}\n",
            " 54% 49/90 [06:17<04:27,  6.52s/it]\u001b[32m2025-12-23 10:05:14.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=49 loss=0.8046 lr=1.02e-04\u001b[0m\n",
            "{'loss': 0.8046, 'grad_norm': 0.33692866563796997, 'learning_rate': 0.00010222222222222222, 'epoch': 7.0}\n",
            " 56% 50/90 [06:25<04:44,  7.12s/it]\u001b[32m2025-12-23 10:05:23.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=50 loss=0.8060 lr=1.00e-04\u001b[0m\n",
            "{'loss': 0.806, 'grad_norm': 0.22723694145679474, 'learning_rate': 0.0001, 'epoch': 7.16}\n",
            " 57% 51/90 [06:34<04:55,  7.58s/it]\u001b[32m2025-12-23 10:05:31.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=51 loss=0.8006 lr=9.78e-05\u001b[0m\n",
            "{'loss': 0.8006, 'grad_norm': 0.26581692695617676, 'learning_rate': 9.777777777777778e-05, 'epoch': 7.32}\n",
            " 58% 52/90 [06:43<05:01,  7.92s/it]\u001b[32m2025-12-23 10:05:40.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=52 loss=0.7972 lr=9.56e-05\u001b[0m\n",
            "{'loss': 0.7972, 'grad_norm': 0.19592757523059845, 'learning_rate': 9.555555555555557e-05, 'epoch': 7.48}\n",
            " 59% 53/90 [06:51<05:00,  8.12s/it]\u001b[32m2025-12-23 10:05:49.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=53 loss=0.7958 lr=9.33e-05\u001b[0m\n",
            "{'loss': 0.7958, 'grad_norm': 0.23422281444072723, 'learning_rate': 9.333333333333334e-05, 'epoch': 7.64}\n",
            " 60% 54/90 [07:00<04:57,  8.25s/it]\u001b[32m2025-12-23 10:05:57.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=54 loss=0.7895 lr=9.11e-05\u001b[0m\n",
            "{'loss': 0.7895, 'grad_norm': 0.2058592289686203, 'learning_rate': 9.111111111111112e-05, 'epoch': 7.8}\n",
            " 61% 55/90 [07:08<04:53,  8.38s/it]\u001b[32m2025-12-23 10:06:06.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=55 loss=0.7905 lr=8.89e-05\u001b[0m\n",
            "{'loss': 0.7905, 'grad_norm': 0.21807706356048584, 'learning_rate': 8.888888888888889e-05, 'epoch': 7.96}\n",
            " 62% 56/90 [07:11<03:41,  6.51s/it]\u001b[32m2025-12-23 10:06:08.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=56 loss=0.7896 lr=8.67e-05\u001b[0m\n",
            "{'loss': 0.7896, 'grad_norm': 0.29056382179260254, 'learning_rate': 8.666666666666667e-05, 'epoch': 8.0}\n",
            " 63% 57/90 [07:19<03:56,  7.17s/it]\u001b[32m2025-12-23 10:06:17.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=57 loss=0.7875 lr=8.44e-05\u001b[0m\n",
            "{'loss': 0.7875, 'grad_norm': 0.15567012131214142, 'learning_rate': 8.444444444444444e-05, 'epoch': 8.16}\n",
            " 64% 58/90 [07:28<04:03,  7.61s/it]\u001b[32m2025-12-23 10:06:25.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=58 loss=0.7834 lr=8.22e-05\u001b[0m\n",
            "{'loss': 0.7834, 'grad_norm': 0.166520893573761, 'learning_rate': 8.222222222222222e-05, 'epoch': 8.32}\n",
            " 66% 59/90 [07:37<04:05,  7.91s/it]\u001b[32m2025-12-23 10:06:34.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=59 loss=0.7792 lr=8.00e-05\u001b[0m\n",
            "{'loss': 0.7792, 'grad_norm': 0.21889837086200714, 'learning_rate': 8e-05, 'epoch': 8.48}\n",
            " 67% 60/90 [07:45<04:03,  8.12s/it]\u001b[32m2025-12-23 10:06:43.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=60 loss=0.7828 lr=7.78e-05\u001b[0m\n",
            "{'loss': 0.7828, 'grad_norm': 0.24844297766685486, 'learning_rate': 7.777777777777778e-05, 'epoch': 8.64}\n",
            " 68% 61/90 [07:54<04:00,  8.29s/it]\u001b[32m2025-12-23 10:06:51.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=61 loss=0.7852 lr=7.56e-05\u001b[0m\n",
            "{'loss': 0.7852, 'grad_norm': 0.236149400472641, 'learning_rate': 7.555555555555556e-05, 'epoch': 8.8}\n",
            " 69% 62/90 [08:02<03:54,  8.38s/it]\u001b[32m2025-12-23 10:07:00.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=62 loss=0.7770 lr=7.33e-05\u001b[0m\n",
            "{'loss': 0.777, 'grad_norm': 0.27352175116539, 'learning_rate': 7.333333333333333e-05, 'epoch': 8.96}\n",
            " 70% 63/90 [08:05<02:56,  6.52s/it]\u001b[32m2025-12-23 10:07:02.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=63 loss=0.7764 lr=7.11e-05\u001b[0m\n",
            "{'loss': 0.7764, 'grad_norm': 0.35154426097869873, 'learning_rate': 7.111111111111112e-05, 'epoch': 9.0}\n",
            " 71% 64/90 [08:13<03:05,  7.15s/it]\u001b[32m2025-12-23 10:07:11.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=64 loss=0.7789 lr=6.89e-05\u001b[0m\n",
            "{'loss': 0.7789, 'grad_norm': 0.1954852044582367, 'learning_rate': 6.88888888888889e-05, 'epoch': 9.16}\n",
            " 72% 65/90 [08:22<03:08,  7.56s/it]\u001b[32m2025-12-23 10:07:19.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=65 loss=0.7788 lr=6.67e-05\u001b[0m\n",
            "{'loss': 0.7788, 'grad_norm': 0.3516257405281067, 'learning_rate': 6.666666666666667e-05, 'epoch': 9.32}\n",
            " 73% 66/90 [08:30<03:08,  7.86s/it]\u001b[32m2025-12-23 10:07:28.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=66 loss=0.7751 lr=6.44e-05\u001b[0m\n",
            "{'loss': 0.7751, 'grad_norm': 0.1804722398519516, 'learning_rate': 6.444444444444446e-05, 'epoch': 9.48}\n",
            " 74% 67/90 [08:39<03:05,  8.07s/it]\u001b[32m2025-12-23 10:07:36.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=67 loss=0.7763 lr=6.22e-05\u001b[0m\n",
            "{'loss': 0.7763, 'grad_norm': 0.1579684019088745, 'learning_rate': 6.222222222222222e-05, 'epoch': 9.64}\n",
            " 76% 68/90 [08:48<03:02,  8.28s/it]\u001b[32m2025-12-23 10:07:45.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=68 loss=0.7725 lr=6.00e-05\u001b[0m\n",
            "{'loss': 0.7725, 'grad_norm': 0.5142143368721008, 'learning_rate': 6e-05, 'epoch': 9.8}\n",
            " 77% 69/90 [08:56<02:56,  8.42s/it]\u001b[32m2025-12-23 10:07:54.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=69 loss=0.7712 lr=5.78e-05\u001b[0m\n",
            "{'loss': 0.7712, 'grad_norm': 0.13487596809864044, 'learning_rate': 5.7777777777777776e-05, 'epoch': 9.96}\n",
            " 78% 70/90 [08:59<02:10,  6.54s/it]\u001b[32m2025-12-23 10:07:56.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=70 loss=0.7717 lr=5.56e-05\u001b[0m\n",
            "{'loss': 0.7717, 'grad_norm': 0.3867757022380829, 'learning_rate': 5.555555555555556e-05, 'epoch': 10.0}\n",
            " 79% 71/90 [09:07<02:16,  7.20s/it]\u001b[32m2025-12-23 10:08:05.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=71 loss=0.7684 lr=5.33e-05\u001b[0m\n",
            "{'loss': 0.7684, 'grad_norm': 0.16574791073799133, 'learning_rate': 5.333333333333333e-05, 'epoch': 10.16}\n",
            " 80% 72/90 [09:16<02:16,  7.61s/it]\u001b[32m2025-12-23 10:08:13.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=72 loss=0.7742 lr=5.11e-05\u001b[0m\n",
            "{'loss': 0.7742, 'grad_norm': 0.4287698566913605, 'learning_rate': 5.111111111111111e-05, 'epoch': 10.32}\n",
            " 81% 73/90 [09:24<02:14,  7.92s/it]\u001b[32m2025-12-23 10:08:22.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=73 loss=0.7702 lr=4.89e-05\u001b[0m\n",
            "{'loss': 0.7702, 'grad_norm': 0.25302088260650635, 'learning_rate': 4.888888888888889e-05, 'epoch': 10.48}\n",
            " 82% 74/90 [09:33<02:09,  8.12s/it]\u001b[32m2025-12-23 10:08:31.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=74 loss=0.7742 lr=4.67e-05\u001b[0m\n",
            "{'loss': 0.7742, 'grad_norm': 0.4017607867717743, 'learning_rate': 4.666666666666667e-05, 'epoch': 10.64}\n",
            " 83% 75/90 [09:42<02:04,  8.29s/it]\u001b[32m2025-12-23 10:08:39.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=75 loss=0.7718 lr=4.44e-05\u001b[0m\n",
            "{'loss': 0.7718, 'grad_norm': 0.21067532896995544, 'learning_rate': 4.4444444444444447e-05, 'epoch': 10.8}\n",
            " 84% 76/90 [09:50<01:56,  8.34s/it]\u001b[32m2025-12-23 10:08:48.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=76 loss=0.7699 lr=4.22e-05\u001b[0m\n",
            "{'loss': 0.7699, 'grad_norm': 0.12210315465927124, 'learning_rate': 4.222222222222222e-05, 'epoch': 10.96}\n",
            " 86% 77/90 [09:52<01:24,  6.49s/it]\u001b[32m2025-12-23 10:08:50.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=77 loss=0.7757 lr=4.00e-05\u001b[0m\n",
            "{'loss': 0.7757, 'grad_norm': 0.22452768683433533, 'learning_rate': 4e-05, 'epoch': 11.0}\n",
            " 87% 78/90 [10:01<01:25,  7.14s/it]\u001b[32m2025-12-23 10:08:58.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=78 loss=0.7727 lr=3.78e-05\u001b[0m\n",
            "{'loss': 0.7727, 'grad_norm': 0.5237349271774292, 'learning_rate': 3.777777777777778e-05, 'epoch': 11.16}\n",
            " 88% 79/90 [10:10<01:23,  7.60s/it]\u001b[32m2025-12-23 10:09:07.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=79 loss=0.7721 lr=3.56e-05\u001b[0m\n",
            "{'loss': 0.7721, 'grad_norm': 0.4229123592376709, 'learning_rate': 3.555555555555556e-05, 'epoch': 11.32}\n",
            " 89% 80/90 [10:18<01:18,  7.88s/it]\u001b[32m2025-12-23 10:09:16.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=80 loss=0.7708 lr=3.33e-05\u001b[0m\n",
            "{'loss': 0.7708, 'grad_norm': 0.30263784527778625, 'learning_rate': 3.3333333333333335e-05, 'epoch': 11.48}\n",
            " 90% 81/90 [10:27<01:12,  8.10s/it]\u001b[32m2025-12-23 10:09:24.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=81 loss=0.7673 lr=3.11e-05\u001b[0m\n",
            "{'loss': 0.7673, 'grad_norm': 0.15942704677581787, 'learning_rate': 3.111111111111111e-05, 'epoch': 11.64}\n",
            " 91% 82/90 [10:35<01:05,  8.25s/it]\u001b[32m2025-12-23 10:09:33.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=82 loss=0.7715 lr=2.89e-05\u001b[0m\n",
            "{'loss': 0.7715, 'grad_norm': 0.21452568471431732, 'learning_rate': 2.8888888888888888e-05, 'epoch': 11.8}\n",
            " 92% 83/90 [10:44<00:58,  8.38s/it]\u001b[32m2025-12-23 10:09:42.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=83 loss=0.7700 lr=2.67e-05\u001b[0m\n",
            "{'loss': 0.77, 'grad_norm': 0.24011041224002838, 'learning_rate': 2.6666666666666667e-05, 'epoch': 11.96}\n",
            " 93% 84/90 [10:46<00:39,  6.51s/it]\u001b[32m2025-12-23 10:09:44.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=84 loss=0.7650 lr=2.44e-05\u001b[0m\n",
            "{'loss': 0.765, 'grad_norm': 0.2549951374530792, 'learning_rate': 2.4444444444444445e-05, 'epoch': 12.0}\n",
            " 94% 85/90 [10:55<00:35,  7.17s/it]\u001b[32m2025-12-23 10:09:52.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=85 loss=0.7659 lr=2.22e-05\u001b[0m\n",
            "{'loss': 0.7659, 'grad_norm': 0.11816069483757019, 'learning_rate': 2.2222222222222223e-05, 'epoch': 12.16}\n",
            " 96% 86/90 [11:04<00:30,  7.57s/it]\u001b[32m2025-12-23 10:10:01.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=86 loss=0.7695 lr=2.00e-05\u001b[0m\n",
            "{'loss': 0.7695, 'grad_norm': 0.23120896518230438, 'learning_rate': 2e-05, 'epoch': 12.32}\n",
            " 97% 87/90 [11:12<00:23,  7.86s/it]\u001b[32m2025-12-23 10:10:09.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=87 loss=0.7687 lr=1.78e-05\u001b[0m\n",
            "{'loss': 0.7687, 'grad_norm': 0.2755938470363617, 'learning_rate': 1.777777777777778e-05, 'epoch': 12.48}\n",
            " 98% 88/90 [11:21<00:16,  8.10s/it]\u001b[32m2025-12-23 10:10:18.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=88 loss=0.7705 lr=1.56e-05\u001b[0m\n",
            "{'loss': 0.7705, 'grad_norm': 0.28769031167030334, 'learning_rate': 1.5555555555555555e-05, 'epoch': 12.64}\n",
            " 99% 89/90 [11:29<00:08,  8.28s/it]\u001b[32m2025-12-23 10:10:27.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=89 loss=0.7684 lr=1.33e-05\u001b[0m\n",
            "{'loss': 0.7684, 'grad_norm': 0.14015927910804749, 'learning_rate': 1.3333333333333333e-05, 'epoch': 12.8}\n",
            "100% 90/90 [11:38<00:00,  8.36s/it]\u001b[32m2025-12-23 10:10:35.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.callbacks.logger\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mstep=90 loss=0.7667 lr=1.11e-05\u001b[0m\n",
            "{'loss': 0.7667, 'grad_norm': 0.12810371816158295, 'learning_rate': 1.1111111111111112e-05, 'epoch': 12.96}\n",
            "{'train_runtime': 700.0484, 'train_samples_per_second': 4.285, 'train_steps_per_second': 0.129, 'train_loss': 1.4444762289524078, 'epoch': 12.96}\n",
            "100% 90/90 [11:40<00:00,  7.78s/it]\n",
            "\u001b[32m2025-12-23 10:10:37.496\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m121\u001b[0m - \u001b[32m\u001b[1mTraining completed\u001b[0m\n",
            "\u001b[32m2025-12-23 10:10:37.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.train.train_model\u001b[0m:\u001b[36mtrain_step\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mSaving model to checkpoints/line_segments/final\u001b[0m\n",
            "\u001b[32m2025-12-23 10:10:41.432\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msteps.train.train_model\u001b[0m:\u001b[36mtrain_step\u001b[0m:\u001b[36m31\u001b[0m - \u001b[32m\u001b[1mModel saved to checkpoints/line_segments/final\u001b[0m\n",
            "\u001b[37mStep \u001b[0m\u001b[38;5;105mtrain_step\u001b[37m has finished in \u001b[0m\u001b[38;5;105m12m20s\u001b[37m.\u001b[0m\n",
            "\u001b[37mStep \u001b[0m\u001b[38;5;105mmerge_lora_step\u001b[37m has started.\u001b[0m\n",
            "\u001b[32m2025-12-23 10:10:48.688\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.train.merge_model\u001b[0m:\u001b[36mmerge_lora_step\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mLoading base model from Qwen/Qwen2.5-3B-Instruct\u001b[0m\n",
            "\u001b[37m[merge_lora_step] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set \u001b[0m\u001b[38;5;105mmax_memory\u001b[37m in to a higher value to use more memory (at your own risk).\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:25<00:00, 12.97s/it]\n",
            "\u001b[33m[merge_lora_step] Some parameters are on the meta device because they were offloaded to the cpu.\u001b[0m\n",
            "\u001b[32m2025-12-23 10:11:15.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.train.merge_model\u001b[0m:\u001b[36mmerge_lora_step\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mLoading LoRA adapter from checkpoints/line_segments/final\u001b[0m\n",
            "\u001b[37m[merge_lora_step] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set \u001b[0m\u001b[38;5;105mmax_memory\u001b[37m in to a higher value to use more memory (at your own risk).\u001b[0m\n",
            "\u001b[32m2025-12-23 10:11:16.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.train.merge_model\u001b[0m:\u001b[36mmerge_lora_step\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mMerging LoRA weights into base model\u001b[0m\n",
            "\u001b[32m2025-12-23 10:11:16.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.train.merge_model\u001b[0m:\u001b[36mmerge_lora_step\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mSaving merged model to checkpoints/line_segments/merged\u001b[0m\n",
            "\u001b[33m[merge_lora_step] /usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:2862: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the \u001b[0m\u001b[38;5;105mshard_size\u001b[33m (5GB default)\n",
            "  warnings.warn(\n",
            "\u001b[0m\n",
            "Saving checkpoint shards: 100% 2/2 [08:35<00:00, 257.60s/it]\n",
            "\u001b[32m2025-12-23 10:19:52.784\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msteps.train.merge_model\u001b[0m:\u001b[36mmerge_lora_step\u001b[0m:\u001b[36m40\u001b[0m - \u001b[32m\u001b[1mMerged model saved to checkpoints/line_segments/merged\u001b[0m\n",
            "\u001b[37mStep \u001b[0m\u001b[38;5;105mmerge_lora_step\u001b[37m has finished in \u001b[0m\u001b[38;5;105m9m8s\u001b[37m.\u001b[0m\n",
            "\u001b[37mPipeline run has finished in \u001b[0m\u001b[38;5;105m25m35s\u001b[37m.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m tools.run --run-inference --no-cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_TJ-DdprGfd",
        "outputId": "3c0a092a-69af-4671-fc7f-6f0daa226f59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[37mNumExpr defaulting to 2 threads.\u001b[0m\n",
            "\u001b[37mTensorFlow version 2.19.0 available.\u001b[0m\n",
            "\u001b[37mJAX version 0.7.2 available.\u001b[0m\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766485223.487505   11134 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766485223.537185   11134 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766485223.904783   11134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766485223.904860   11134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766485223.904865   11134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766485223.904869   11134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32m2025-12-23 10:20:33.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mLoading inference config from /content/Capstone/configs/inference.yaml\u001b[0m\n",
            "\u001b[32m2025-12-23 10:20:33.739\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m151\u001b[0m - \u001b[33m\u001b[1mTrained model not found at /checkpoints/line_segments/merged, will use base Qwen model\u001b[0m\n",
            "\u001b[32m2025-12-23 10:20:33.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStarting inference pipeline\u001b[0m\n",
            "\u001b[32m2025-12-23 10:20:33.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m160\u001b[0m - \u001b[1mModel: Base Qwen2.5-3B-Instruct\u001b[0m\n",
            "\u001b[32m2025-12-23 10:20:33.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mPrompt: Vẽ đoạn thẳng CD có độ dài 8\u001b[0m\n",
            "\u001b[32m2025-12-23 10:20:33.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipelines.inference\u001b[0m:\u001b[36minference_pipeline\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mStarting inference pipeline\u001b[0m\n",
            "\u001b[32m2025-12-23 10:20:33.746\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mpipelines.inference\u001b[0m:\u001b[36minference_pipeline\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mInference completed: <zenml.steps.entrypoint_function_utils.StepArtifact object at 0x7a07036338c0>\u001b[0m\n",
            "\u001b[37mInitiating a new run for the pipeline: \u001b[0m\u001b[38;5;105minference_pipeline\u001b[37m.\u001b[0m\n",
            "\u001b[37mCaching is disabled by default for \u001b[0m\u001b[38;5;105minference_pipeline\u001b[37m.\u001b[0m\n",
            "\u001b[37mUsing user: \u001b[0m\u001b[38;5;105mfinetune\u001b[37m\u001b[0m\n",
            "\u001b[37mUsing stack: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
            "\u001b[37m  orchestrator: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
            "\u001b[37m  artifact_store: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
            "\u001b[37m  deployer: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
            "\u001b[37mDashboard URL for Pipeline Run: \u001b[0m\u001b[34mhttps://victoria-communicable-sometimes.ngrok-free.dev/projects/default/runs/64e8225f-ed50-4834-8f4f-d8b6329d89f3\u001b[37m\u001b[0m\n",
            "\u001b[37mStep \u001b[0m\u001b[38;5;105mtest_inference_step\u001b[37m has started.\u001b[0m\n",
            "\u001b[32m2025-12-23 10:20:51.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.inference.test_inference\u001b[0m:\u001b[36mtest_inference_step\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mLoading models for inference test\u001b[0m\n",
            "\u001b[32m2025-12-23 10:20:51.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.inference.test_inference\u001b[0m:\u001b[36mtest_inference_step\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mLoading model from: Qwen/Qwen2.5-3B-Instruct\u001b[0m\n",
            "\u001b[32m2025-12-23 10:20:51.795\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msteps.inference.test_inference\u001b[0m:\u001b[36mtest_inference_step\u001b[0m:\u001b[36m33\u001b[0m - \u001b[33m\u001b[1mNo trained model found, using base Qwen model (not fine-tuned)\u001b[0m\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "\u001b[37m[test_inference_step] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set \u001b[0m\u001b[38;5;105mmax_memory\u001b[37m in to a higher value to use more memory (at your own risk).\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:25<00:00, 12.67s/it]\n",
            "config.json: 100% 67.0/67.0 [00:00<00:00, 255kB/s]\n",
            "pytorch_model.safetensors: 100% 696M/696M [00:06<00:00, 114MB/s]\n",
            "\u001b[32m2025-12-23 10:21:25.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.inference.test_inference\u001b[0m:\u001b[36mtest_inference_step\u001b[0m:\u001b[36m54\u001b[0m - \u001b[1mTest prompt: Vẽ đoạn thẳng CD có độ dài 8\u001b[0m\n",
            "\u001b[32m2025-12-23 10:21:25.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.inference.test_inference\u001b[0m:\u001b[36mtest_inference_step\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mGenerating image tokens\u001b[0m\n",
            "\u001b[32m2025-12-23 10:22:21.309\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msteps.inference.test_inference\u001b[0m:\u001b[36mtest_inference_step\u001b[0m:\u001b[36m98\u001b[0m - \u001b[33m\u001b[1mCould not extract image tokens with markers: 151667 is not in list\u001b[0m\n",
            "\u001b[32m2025-12-23 10:22:21.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.inference.test_inference\u001b[0m:\u001b[36mtest_inference_step\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mUsing fallback: first 1024 generated tokens\u001b[0m\n",
            "\u001b[32m2025-12-23 10:22:21.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.inference.test_inference\u001b[0m:\u001b[36mtest_inference_step\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mDecoding tokens to image, shape: torch.Size([1, 1024])\u001b[0m\n",
            "\u001b[32m2025-12-23 10:22:23.835\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msteps.inference.test_inference\u001b[0m:\u001b[36mtest_inference_step\u001b[0m:\u001b[36m121\u001b[0m - \u001b[32m\u001b[1mTest image generated: /content/Capstone/outputs/inference_test/test_generated.png\u001b[0m\n",
            "\u001b[37mStep \u001b[0m\u001b[38;5;105mtest_inference_step\u001b[37m has finished in \u001b[0m\u001b[38;5;105m1m35s\u001b[37m.\u001b[0m\n",
            "\u001b[37mPipeline run has finished in \u001b[0m\u001b[38;5;105m1m39s\u001b[37m.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reset"
      ],
      "metadata": {
        "id": "ccTEfkkM81Fg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwYOVxmfERs8"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Capstone"
      ],
      "metadata": {
        "id": "g8ZVsAKe83BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "pA6JgUri84-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0k6InMQiCG7A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}