{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWdb9Dcl-x8v",
        "outputId": "19da50ca-bcbd-4246-91a0-7ea7dad00d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Capstone'...\n",
            "remote: Enumerating objects: 6610, done.\u001b[K\n",
            "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 6610 (delta 41), reused 78 (delta 26), pack-reused 6500 (from 3)\u001b[K\n",
            "Receiving objects: 100% (6610/6610), 506.09 MiB | 15.57 MiB/s, done.\n",
            "Resolving deltas: 100% (102/102), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/johnpham4/Capstone.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zDUa5cr_SAQ",
        "outputId": "57afad57-97c8-467a-ce88-c8b59c5b7ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Capstone\n"
          ]
        }
      ],
      "source": [
        "%cd Capstone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQyJsOYr_bTx",
        "outputId": "6cfef6e7-1035-4092-9d90-e1a97798a2da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: python-doctr 1.0.0 does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for geouni (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-core 0.1.23 requires packaging<24.0,>=23.2, but you have packaging 25.0 which is incompatible.\n",
            "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.1.23 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "langgraph-checkpoint 3.0.1 requires langchain-core>=0.2.38, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cmRS3x-Evs-",
        "outputId": "54578e7c-4dff-4c8c-f924-dd8284903798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.3/803.3 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/241.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "zenml 0.91.2 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
            "google-adk 1.21.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.45.3 which is incompatible.\n",
            "google-adk 1.21.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.\n",
            "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.1.23 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "langgraph-checkpoint 3.0.1 requires langchain-core>=0.2.38, but you have langchain-core 0.1.23 which is incompatible.\n",
            "db-dtypes 1.4.4 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "xarray 2025.12.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain==0.0.354 langchain-community==0.0.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-UJCFnL_fAM",
        "outputId": "7195ddba-daee-4139-8c9e-932fd82ac627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mThe `zenml connect` command is deprecated and will be removed in a future \u001b[0m\n",
            "\u001b[33mrelease. Please use the `zenml login` command instead. \u001b[0m\n",
            "\u001b[2;37mCalling `zenml login`\u001b[0m\u001b[2;33m...\u001b[0m\n",
            "Please enter the API key for the ZenML server: \n",
            "\u001b[37mInitializing the ZenML global configuration version to 0.91.2\u001b[0m\n",
            "\u001b[2;37mAuthenticating to ZenML server \u001b[0m\n",
            "\u001b[37m'https://victoria-communicable-sometimes.ngrok-free.dev'\u001b[0m\u001b[2;37m using an API key\u001b[0m\u001b[2;33m...\u001b[0m\n",
            "\u001b[37mSetting the global active project to 'default'.\u001b[0m\n",
            "\u001b[33mSetting the global active stack to default.\u001b[0m\n",
            "\u001b[37mUpdated the global store configuration.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!zenml connect \\\n",
        "  --url https://victoria-communicable-sometimes.ngrok-free.dev \\\n",
        "  --api-key ZENKEY_eyJpZCI6ImUxOGYwNjQ1LWRjZmEtNGQwNS04NWNkLTVkZGNjYzlmZDg5NSIsImtleSI6IjQ3YmU1MGQ5NWNiOWU4NmY2YTI5NWYxNGY5ZjM4NmE3M2IzYWIwZTZhYTk4NDA2M2E5MzI3OWUwYTU5YWI4ZTgifQ=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQmjok_HCI0r",
        "outputId": "94031cb6-64c7-4c2e-9527-ece43cbd327b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2;37m-----ZenML Client Status-----\u001b[0m\n",
            "\u001b[2;37mConnected to a remote ZenML server: \u001b[0m\n",
            "\u001b[2;37m`\u001b[0m\u001b]8;id=609672;https://victoria-communicable-sometimes.ngrok-free.dev\u001b\\\u001b[2;4;94mhttps://victoria-communicable-sometimes.ngrok-free.dev\u001b[0m\u001b]8;;\u001b\\\u001b[2;4;94m`\u001b[0m\n",
            "\u001b[2;37m  Dashboard: \u001b[0m\u001b]8;id=580542;https://victoria-communicable-sometimes.ngrok-free.dev\u001b\\\u001b[2;4;94mhttps://victoria-communicable-sometimes.ngrok-free.dev\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;37m  API: \u001b[0m\u001b]8;id=890527;https://victoria-communicable-sometimes.ngrok-free.dev/docs\u001b\\\u001b[2;4;94mhttps://victoria-communicable-sometimes.ngrok-free.dev\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;37m  Server status: \u001b[0m\u001b[37m'available'\u001b[0m\n",
            "\u001b[2;37m  Server authentication: API key\u001b[0m\n",
            "\u001b[2;37m  The active user is: \u001b[0m\u001b[37m'finetune'\u001b[0m\n",
            "\u001b[2;37m  The active project is: \u001b[0m\u001b[37m'default'\u001b[0m\u001b[2;37m \u001b[0m\u001b[1;2;37m(\u001b[0m\u001b[2;37mglobal\u001b[0m\u001b[1;2;37m)\u001b[0m\n",
            "\u001b[2;37m  The active stack is: \u001b[0m\u001b[37m'default'\u001b[0m\u001b[2;37m \u001b[0m\u001b[1;2;37m(\u001b[0m\u001b[2;37mglobal\u001b[0m\u001b[1;2;37m)\u001b[0m\n",
            "\u001b[2;37mUsing configuration from: \u001b[0m\u001b[37m'/root/.config/zenml'\u001b[0m\n",
            "\u001b[2;37mLocal store files are located at: \u001b[0m\u001b[37m'/root/.config/zenml/local_stores'\u001b[0m\n",
            "\n",
            "\u001b[2;37m-----Local ZenML Server Status-----\u001b[0m\n",
            "\u001b[2;37mThe local server has not been started.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!zenml status"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python tools/draw_line_segments.py --samples 400"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzkr1_ux0Qmq",
        "outputId": "f3a48bdd-6871-48fd-ccb5-bb1dc4dcce16"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2025-12-22 18:23:49.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1mGenerating 400 line segments to ./datasett/line_segments\u001b[0m\n",
            "\u001b[32m2025-12-22 18:23:51.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_line_segments\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mGenerated 50/400 line segments\u001b[0m\n",
            "\u001b[32m2025-12-22 18:23:53.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_line_segments\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mGenerated 100/400 line segments\u001b[0m\n",
            "\u001b[32m2025-12-22 18:23:56.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_line_segments\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mGenerated 150/400 line segments\u001b[0m\n",
            "\u001b[32m2025-12-22 18:23:59.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_line_segments\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mGenerated 200/400 line segments\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:01.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_line_segments\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mGenerated 250/400 line segments\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:03.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_line_segments\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mGenerated 300/400 line segments\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:06.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_line_segments\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mGenerated 350/400 line segments\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:08.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_line_segments\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mGenerated 400/400 line segments\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:08.528\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_line_segments\u001b[0m:\u001b[36m117\u001b[0m - \u001b[32m\u001b[1mCompleted 400 line segments\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:08.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_line_segments\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mImages: datasett/line_segments/images\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:08.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_line_segments\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mDataset: datasett/line_segments/dataset.json\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj-jSTFEDPL9",
        "outputId": "d3b9b2a6-fd78-489e-88f0-ceaf5f7ae46b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[37mNumExpr defaulting to 2 threads.\u001b[0m\n",
            "\u001b[37mTensorFlow version 2.19.0 available.\u001b[0m\n",
            "\u001b[37mJAX version 0.7.2 available.\u001b[0m\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766427859.762686   32369 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766427859.770877   32369 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766427859.803821   32369 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766427859.803853   32369 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766427859.803859   32369 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766427859.803862   32369 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32m2025-12-22 18:24:27.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mLoading config from /content/Capstone/configs/training.yaml\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:27.204\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m105\u001b[0m - \u001b[33m\u001b[1mTest image not found: /content/Capstone/datasett/line_segments/images/0000_AB.png\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:27.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mStarting training pipeline\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:27.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mMerge model: True\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:27.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mPush to hub: False\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:27.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mTest inference: False\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:27.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipelines.training\u001b[0m:\u001b[36mtraining_pipeline\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mStarting training pipeline\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:27.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipelines.training\u001b[0m:\u001b[36mtraining_pipeline\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mMerging LoRA adapter\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:27.214\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mpipelines.training\u001b[0m:\u001b[36mtraining_pipeline\u001b[0m:\u001b[36m58\u001b[0m - \u001b[32m\u001b[1mPipeline completed\u001b[0m\n",
            "\u001b[37mInitiating a new run for the pipeline: \u001b[0m\u001b[38;5;105mtraining_pipeline\u001b[37m.\u001b[0m\n",
            "\u001b[37mCaching is disabled by default for \u001b[0m\u001b[38;5;105mtraining_pipeline\u001b[37m.\u001b[0m\n",
            "\u001b[37mUsing user: \u001b[0m\u001b[38;5;105mfinetune\u001b[37m\u001b[0m\n",
            "\u001b[37mUsing stack: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
            "\u001b[37m  orchestrator: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
            "\u001b[37m  artifact_store: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
            "\u001b[37m  deployer: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
            "\u001b[37mDashboard URL for Pipeline Run: \u001b[0m\u001b[34mhttps://victoria-communicable-sometimes.ngrok-free.dev/projects/default/runs/0a047e11-7ec2-4340-b39a-640f0c6aa49a\u001b[37m\u001b[0m\n",
            "\u001b[37mStep \u001b[0m\u001b[38;5;105mload_model_step\u001b[37m has started.\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:47.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.train.load_model\u001b[0m:\u001b[36mload_model_step\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mLoading model\u001b[0m\n",
            "\u001b[32m2025-12-22 18:24:47.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mLoading LLM: Qwen/Qwen2.5-3B-Instruct\u001b[0m\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "\u001b[37m[load_model_step] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set \u001b[0m\u001b[38;5;105mmax_memory\u001b[37m in to a higher value to use more memory (at your own risk).\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:32<00:00, 16.38s/it]\n",
            "\u001b[32m2025-12-22 18:25:25.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mLoading VQ-VAE: JO-KU/Geo-MAGVIT\u001b[0m\n",
            "\u001b[32m2025-12-22 18:25:27.149\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m64\u001b[0m - \u001b[32m\u001b[1mModel loaded successfully\u001b[0m\n",
            "[load_model_step] trainable params: 3,686,400 || all params: 3,089,625,088 || trainable%: 0.1193\n",
            "\u001b[33m[load_model_step] No materializer is registered for type \u001b[0m\u001b[38;5;105m<class 'llm_engineering.applications.training.model_trainer.ModelTrainer'>\u001b[33m, so the default Pickle materializer was used. Pickle is not production ready and should only be used for prototyping as the artifacts cannot be loaded when running with a different Python version. Please consider implementing a custom materializer for type \u001b[0m\u001b[38;5;105m<class 'llm_engineering.applications.training.model_trainer.ModelTrainer'>\u001b[33m according to the instructions at \u001b[0m\u001b[34mhttps://docs.zenml.io/concepts/artifacts/materializers\u001b[33m\u001b[0m\n",
            "\u001b[37mStep \u001b[0m\u001b[38;5;105mload_model_step\u001b[37m has finished in \u001b[0m\u001b[38;5;105m3m57s\u001b[37m.\u001b[0m\n",
            "\u001b[37mStep \u001b[0m\u001b[38;5;105mtrain_step\u001b[37m has started.\u001b[0m\n",
            "\u001b[32m2025-12-22 18:29:24.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.train.train_model\u001b[0m:\u001b[36mtrain_step\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mLoading datasets\u001b[0m\n",
            "\u001b[32m2025-12-22 18:29:24.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mLoading dataset from /content/Capstone/datasett/line_segments/dataset.json\u001b[0m\n",
            "\u001b[32m2025-12-22 18:29:24.182\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m77\u001b[0m - \u001b[32m\u001b[1mLoaded 400 training samples\u001b[0m\n",
            "\u001b[32m2025-12-22 18:29:24.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msteps.train.train_model\u001b[0m:\u001b[36mtrain_step\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mStarting training\u001b[0m\n",
            "\u001b[32m2025-12-22 18:29:24.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_engineering.applications.training.model_trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mStarting training\u001b[0m\n",
            "\u001b[33m[train_step] /content/Capstone/llm_engineering/applications/training/model_trainer.py:113: FutureWarning: \u001b[0m\u001b[38;5;105mtokenizer\u001b[33m is deprecated and will be removed in version 5.0.0 for \u001b[0m\u001b[38;5;105mTrainer.__init__\u001b[33m. Use \u001b[0m\u001b[38;5;105mprocessing_class\u001b[33m instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[0m\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "  0% 0/300 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "\u001b[33m[train_step] /usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "\u001b[0m\n",
            "  0% 1/300 [01:49<9:06:01, 109.57s/it]"
          ]
        }
      ],
      "source": [
        "!python -m tools.run --run-train --no-cache"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reset"
      ],
      "metadata": {
        "id": "ccTEfkkM81Fg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "kwYOVxmfERs8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28abb026-a80e-4f21-9772-a24ce11aa286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Capstone"
      ],
      "metadata": {
        "id": "g8ZVsAKe83BP"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA6JgUri84-B",
        "outputId": "ef15b550-2250-40f8-87b5-b778f344aebe"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0k6InMQiCG7A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}