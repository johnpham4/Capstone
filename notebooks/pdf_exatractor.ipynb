{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl2BEwZHkxOJ",
        "outputId": "8e93d86f-e04c-4048-ffbe-687467a47902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m852.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m842.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.2/978.2 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 ultralytics python-doctr PyMuPDF pillow einops numpy transformers peft diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import fitz\n",
        "import os\n",
        "import pathlib\n",
        "from PIL import Image, ImageEnhance\n",
        "import numpy as np\n",
        "import os\n",
        "from doctr.io import DocumentFile\n",
        "from doctr.models import ocr_predictor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7t1CcSOkzwl",
        "outputId": "72a9e8c4-96c0-4840-d65d-58105911cb39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://huggingface.co/DILHTWD/documentlayoutsegmentation_YOLOv8_ondoclaynet/resolve/main/yolov8x-doclaynet-epoch64-imgsz640-initiallr1e-4-finallr1e-5.pt"
      ],
      "metadata": {
        "id": "RHmPgSxRtICs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of sample PDF files to process\n",
        "pdf_list = ['test.pdf']\n",
        "# Load the document segmentation model\n",
        "docseg_model = YOLO('yolov8x-doclaynet-epoch64-imgsz640-initiallr1e-4-finallr1e-5.pt')\n",
        "# Initialize a dictionary to store results\n",
        "mydict = {}\n",
        "\n",
        "def enhance_image(img):\n",
        "    \"\"\"Apply image enhancements for better quality.\"\"\"\n",
        "    # Enhance sharpness\n",
        "    enhancer = ImageEnhance.Sharpness(img)\n",
        "    img = enhancer.enhance(1.5)\n",
        "\n",
        "    # Enhance contrast\n",
        "    enhancer = ImageEnhance.Contrast(img)\n",
        "    img = enhancer.enhance(1.2)\n",
        "\n",
        "    # Enhance color\n",
        "    enhancer = ImageEnhance.Color(img)\n",
        "    img = enhancer.enhance(1.1)\n",
        "\n",
        "    return img\n",
        "\n",
        "def process_pdf_page(pdf_path, page_num, docseg_model, output_dir):\n",
        "    \"\"\"Processes a single page of a PDF with maximum quality settings.\"\"\"\n",
        "\n",
        "    pdf_doc = fitz.open(pdf_path)\n",
        "    page = pdf_doc[page_num]\n",
        "\n",
        "     # Increase the resolution matrix for maximum quality\n",
        "    zoom = 4  # Increased zoom factor for higher resolution\n",
        "    matrix = fitz.Matrix(zoom, zoom)\n",
        "\n",
        "    # Use high-quality rendering options\n",
        "    pix = page.get_pixmap(\n",
        "        matrix=matrix,\n",
        "        alpha=False,  # Disable alpha channel for clearer images\n",
        "        colorspace=fitz.csRGB  # Force RGB colorspace\n",
        "    )\n",
        "\n",
        "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "\n",
        "    # Apply image enhancements\n",
        "    img = enhance_image(img)\n",
        "\n",
        "    # Resize with high-quality settings\n",
        "    if zoom != 1:\n",
        "        original_size = (int(page.rect.width), int(page.rect.height))\n",
        "        img = img.resize(original_size, Image.Resampling.LANCZOS)\n",
        "    # Generate a temporary filename for the page image\n",
        "    temp_img_filename = os.path.join(output_dir, f\"temp_page_{page_num}.png\")\n",
        "\n",
        "    # Save with maximum quality settings\n",
        "    img.save(\n",
        "        temp_img_filename,\n",
        "        \"PNG\",\n",
        "        quality=100,\n",
        "        optimize=False,\n",
        "        dpi=(300, 300)  # Set high DPI\n",
        "    )\n",
        "    # Run the model on the image\n",
        "    results = docseg_model(source=temp_img_filename, save=True, show_labels=True, show_conf=True, boxes=True)\n",
        "    # Extract the results\n",
        "    page_width = page.rect.width\n",
        "    one_third_width = page_width / 3\n",
        "\n",
        "\n",
        "    all_coords = []\n",
        "\n",
        "    for entry in results:\n",
        "        thepath = pathlib.Path(entry.path)\n",
        "        thecoords = entry.boxes.xyxy.numpy()\n",
        "        all_coords.extend(thecoords)\n",
        "\n",
        "    # Sort the coordinates into two groups and then sort each group by y1\n",
        "    left_group = []\n",
        "    right_group = []\n",
        "    for bbox in all_coords:\n",
        "            x1 = bbox[0]\n",
        "            if x1 < one_third_width:\n",
        "                left_group.append(bbox)\n",
        "            else:\n",
        "                right_group.append(bbox)\n",
        "\n",
        "    left_group = sorted(left_group, key=lambda bbox: bbox[1])\n",
        "    right_group = sorted(right_group, key=lambda bbox: bbox[1])\n",
        "\n",
        "    sorted_coords = left_group + right_group\n",
        "\n",
        "    mydict[f\"{pdf_path} Page {page_num}\"] = sorted_coords\n",
        "    # Clean up the temporary image\n",
        "    os.remove(temp_img_filename)\n",
        "    pdf_doc.close()\n",
        "\n",
        "\n",
        "# Process each PDF in the list\n",
        "for pdf_path in pdf_list:\n",
        "    try:\n",
        "        pdf_doc = fitz.open(pdf_path)\n",
        "        num_pages = pdf_doc.page_count\n",
        "        pdf_doc.close()\n",
        "        output_dir = os.path.splitext(pdf_path)[0] + \"_output\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        for page_num in range(num_pages):\n",
        "            process_pdf_page(pdf_path, page_num, docseg_model, output_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdf_path}: {e}\")\n",
        "# Create the 'tmp' directory if it doesn't exist\n",
        "tmp_dir = 'tmp'\n",
        "os.makedirs(tmp_dir, exist_ok=True)\n",
        "# Iterate through the results and save cropped images with maximum quality\n",
        "for key, coords in mydict.items():\n",
        "    pdf_name, page_info = key.split(\" Page \")\n",
        "    page_number = int(page_info)\n",
        "    pdf_doc = fitz.open(pdf_name)\n",
        "    page = pdf_doc[page_number]\n",
        "\n",
        "    zoom = 4\n",
        "    matrix = fitz.Matrix(zoom,zoom)\n",
        "    for i, bbox in enumerate(coords):\n",
        "        # Scale the bounding box coordinates appropriately\n",
        "        xmin, ymin, xmax, ymax = map(lambda x: x , bbox)\n",
        "\n",
        "        # Create a rectangle from the bounding box\n",
        "        rect = fitz.Rect(xmin, ymin, xmax, ymax)\n",
        "\n",
        "        # Crop using get_pixmap with a maximum resolution matrix\n",
        "        cropped_pix = page.get_pixmap(\n",
        "            clip=rect,\n",
        "            matrix=matrix,\n",
        "            alpha=False,\n",
        "            colorspace=fitz.csRGB\n",
        "        )\n",
        "\n",
        "        cropped_img = Image.frombytes(\"RGB\", [cropped_pix.width, cropped_pix.height], cropped_pix.samples)\n",
        "        cropped_img = enhance_image(cropped_img)\n",
        "\n",
        "        output_filename = os.path.join(tmp_dir, f\"{os.path.splitext(os.path.basename(pdf_name))[0]}_page{page_number}_{i}.png\")\n",
        "\n",
        "        # Save the cropped image\n",
        "        cropped_img.save(output_filename, \"PNG\", quality=100, optimize=False, dpi=(300, 300))\n",
        "    pdf_doc.close()\n",
        "\n",
        "def extract_text_from_image(image_path, model):\n",
        "    \"\"\"Extracts text from a single image using DocTr.\"\"\"\n",
        "    doc = DocumentFile.from_images(image_path)\n",
        "    result = model(doc)\n",
        "    text_content = \"\"\n",
        "    for page in result.pages:\n",
        "        for block in page.blocks:\n",
        "            for line in block.lines:\n",
        "                for word in line.words:\n",
        "                    text_content += word.value + \" \"\n",
        "            text_content += \"\\n\"\n",
        "    return text_content.strip()\n",
        "\n",
        "def process_cropped_images(tmp_dir, pdf_list):\n",
        "    \"\"\"Iterates through cropped images, extracts text using DocTr and stores the text in text files.\"\"\"\n",
        "\n",
        "    doctr_model = ocr_predictor(pretrained=True)\n",
        "\n",
        "    for pdf_path in pdf_list:\n",
        "        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "        output_txt_path = f\"{pdf_name}_extracted_text.txt\"\n",
        "\n",
        "        with open(output_txt_path, 'w', encoding='utf-8') as outfile:\n",
        "\n",
        "            pdf_doc = fitz.open(pdf_path)\n",
        "            num_pages = pdf_doc.page_count\n",
        "            pdf_doc.close()\n",
        "            for page_num in range(num_pages):\n",
        "\n",
        "                outfile.write(f\"Page: {page_num}\\n\")\n",
        "\n",
        "                # Sort filenames of cropped images by chunk order\n",
        "                cropped_images_for_page = sorted([\n",
        "                    f for f in os.listdir(tmp_dir)\n",
        "                    if f.startswith(f\"{pdf_name}_page{page_num}_\") and f.endswith(\".png\")\n",
        "                ], key=lambda f: int(f.split(\"_\")[-1].split(\".\")[0]))\n",
        "\n",
        "                for i, image_filename in enumerate(cropped_images_for_page):\n",
        "                    image_path = os.path.join(tmp_dir, image_filename)\n",
        "                    text = extract_text_from_image(image_path, doctr_model)\n",
        "                    outfile.write(f\"  Chunk {i}: {text}\\n\")\n",
        "        print(f\"Text extracted from {pdf_name} saved to {output_txt_path}\")\n",
        "\n",
        "# Example usage:\n",
        "tmp_dir = 'tmp' # Make sure your tmp directory exists\n",
        "pdf_list = ['test.pdf'] # Your list of PDFs\n",
        "process_cropped_images(tmp_dir, pdf_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLHGPNcCkyNU",
        "outputId": "93a84c04-304b-4eee-c897-4ade0732024e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ 'boxes' is deprecated and will be removed in the future. Use 'show_boxes' instead.\n",
            "\n",
            "image 1/1 /content/test_output/temp_page_0.png: 640x512 1 Caption, 1 List-item, 1 Page-footer, 2 Pictures, 1 Section-header, 11 Texts, 4344.4ms\n",
            "Speed: 4.3ms preprocess, 4344.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1m/content/runs/detect/predict2\u001b[0m\n",
            "WARNING ⚠️ 'boxes' is deprecated and will be removed in the future. Use 'show_boxes' instead.\n",
            "\n",
            "image 1/1 /content/test_output/temp_page_1.png: 640x512 6 List-items, 1 Page-footer, 2 Pictures, 1 Section-header, 3395.6ms\n",
            "Speed: 4.0ms preprocess, 3395.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1m/content/runs/detect/predict2\u001b[0m\n",
            "Text extracted from test saved to test_extracted_text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AI_9Wa-tspMB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}